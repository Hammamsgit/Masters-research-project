{"cells":[{"cell_type":"markdown","source":["# Descriptive summary: Binary experiment (DDoS)\n","\n","In this notebook we build a cGAN model that generates DDoS and Benign observations. This includes:\n","                            \n","\n","1.   Building a discriminator \n","2.   Building a generator \n","3.   Combining the generator and discriminator model to update the generator\n","4.   Saving an initial testing quality of generated data\n","\n","Then we generate data with our best cGAN model and use it to train a Random Forest model. Thereafter, we compare the performance of the aforementioned model to that of Random Forest trained on a real dataset. \n","\n"],"metadata":{"id":"aHBQPQ8dLxLh"}},{"cell_type":"markdown","source":["# Import relevant libraries "],"metadata":{"id":"yPxfbM0POzXA"}},{"cell_type":"code","source":["# connect to google drive for storing and retrieving data and model \n","from google.colab import drive\n","\n","#  modules for importing and manipulating data\n","import pandas as pd \n","import re \n","\n","# modules used throughout the gans process (mainly feeding noise and real/fake labels to generator)\n","import numpy as np \n","from numpy.random import randint,randn\n","from numpy import(\n","    expand_dims,\n","    zeros,\n","    ones,\n","    asarray\n",")\n","\n","# modules used to build gans (mainly diffrent types of layers, regularisation and optimisers)\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras import backend\n","from tensorflow.keras.initializers import RandomNormal\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import (\n","      Input,\n","      BatchNormalization,\n","      Dense,\n","      Reshape,\n","      Flatten,\n","      LeakyReLU,\n","      Dropout,\n","      Lambda,\n","      Activation,\n","      Embedding,\n","      Concatenate,\n","      multiply\n",")\n","\n","\n","\n","# modules for data transformation, sampling and modelling \n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# modules for scoring and evaluating performance\n","from sklearn.metrics import (\n","    roc_curve,\n","    roc_auc_score,\n","    confusion_matrix,\n","    classification_report,\n","    accuracy_score,\n","    f1_score\n",")"],"metadata":{"id":"HJJrRgOPLtH6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import data from drive\n","\n","Note: \n","\n","\n","1. Data was previously downloaded from source: https://www.unb.ca/cic/datasets/ids-2017.html\n","\n","2. Pre-processing: the data was pre-processed as was mentioned in the main text. \n","\n","3. Down sampled to only include 8k observations per class. This was due to the storage and computation constraints experienced when working with colab."],"metadata":{"id":"vhKq89gmTmni"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zm-NfTjENU1J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647114455424,"user_tz":0,"elapsed":1623,"user":{"displayName":"Hammam Alzilitni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00836050819777530402"}},"outputId":"8fd6c3ac-83b4-412f-bbf3-750abf8197f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# mount drive \n","drive.mount('/content/drive')\n","\n","# import data from drive \n","data_extension = '/content/drive/MyDrive/RESEARCH-PROJECT/Binary_experiments/Datasets/2017ddos.parquet'\n","df1 = pd.read_parquet(data_extension, engine='auto')"]},{"cell_type":"markdown","source":["# Split data in to training and testing samples"],"metadata":{"id":"4bh1QZ7hVpbi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6N-hlYE8KWK3"},"outputs":[],"source":["# train, test split (75/25)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df1.drop([\"attack_map\",\"Label\"], axis=1), df1['attack_map'], test_size=0.25, random_state=42,stratify=df1['attack_map']\n",")"]},{"cell_type":"markdown","source":["# Scale features to be between 0-1\n","\n","\n","Standardisation is a form of scaling that standardises all value ranges within our dataset to be between 0-1.\n","\n"," In doing so, this:\n","\n","\n","1. speeds up convergence \n","2. creates a more stable training process \n"],"metadata":{"id":"170E4kcKdoLS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F64xg3bRNVio"},"outputs":[],"source":["# Scale all features to be between 0-1 this is to ensure all features are viewed with equal importance by the algorithm\n","scaler = StandardScaler()\n","\n","# Make sure to only fit the scaler on to the training data\n","X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\n","X_test[X_train.columns] = scaler.transform(X_test[X_train.columns])"]},{"cell_type":"markdown","source":["# Build discriminator\n","1.   We have two input layers, one for the features and one for the labels.\n","\n","2. An embedding layer is used to encode the label input into a 67-dimensional vector, i.e. one dimension per feature.\n","\n","3.  We then use the multiply layer to condition features on the label embedding. This is referred to as model input.\n","\n","4.  This \"model input\" is fed through 3 hidden layers.\n","\n","5.   We also use dropout after the second and third layer, to discourage overfitting.\n","\n","6.  Finally, we have one output layer that distinguishes between real and generated observations.\n","\n","\n","\n"],"metadata":{"id":"gw-3Z3oLdr-c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYfIgxlJKgwM"},"outputs":[],"source":["# define the supervised discriminator model\n","def define_discriminator(out_shape=67, num_classes=2):\n","    # Initialiser that generates tensors with a normal distribution\n","    init = RandomNormal(mean=0.0, stddev=0.02)\n","     \n","    # label input \n","    label = Input(shape=(1,), dtype='int64', name=\"First_input_layer\")\n","\n","    # embed and flatten labels: we then get vector of shape 1 x output shape \n","    label_embedding = Flatten()(Embedding(num_classes, out_shape)(label))\n","    \n","    # feed features through the second input layer \n","    gen_sample = Input(shape=(out_shape,),name=\"Second_input_layer\")\n","    \n","    # condition the discrimination of generated features\n","    model_input = multiply([gen_sample, label_embedding])\n","    \n","    # our first hidden layer and leaky relu activation\n","    fe = Dense(units=512, kernel_initializer=init, name=\"First_hidden_layer\")(model_input)\n","    fe = LeakyReLU(alpha=0.2)(fe)\n","    \n","    # our second hidden layer and leaky relu activation\n","    fe = Dense(256, kernel_initializer=init, name=\"Second_hidden_layer\")(fe)\n","    fe = LeakyReLU(alpha=0.2)(fe)\n","\n","    # apply dropout\n","    fe = Dropout(0.4)(fe)\n","\n","    # our third hidden layer and leaky relu activation\n","    fe = Dense(68, kernel_initializer=init, name=\"Third_hidden_layer\")(fe)\n","    fe = LeakyReLU(alpha=0.2)(fe)\n","    \n","    # apply dropout\n","    fe = Dropout(0.4)(fe)\n","    \n","    # our output layer, with a single node (outputting the probability of an observation being real)\n","    fe = Dense(1,name=\"Output_layer\")(fe)\n","\n","    # sigmoid activation function \n","    out_layer= Activation('sigmoid')(fe)\n","   \n","    # model layer groups layers into an object with training and inference features.\n","    model = Model([gen_sample, label], out_layer)\n","    \n","    # compile model \n","    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.02, beta_1=0.5),metrics=['accuracy'])\n","\n","    # save model architecture plot \n","    dot_img_file = '/content/drive/MyDrive/RESEARCH-PROJECT/Binary_experiments/Models/Binary_discriminator.png'\n","    tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n","  \n","    return model\n"," "]},{"cell_type":"markdown","source":["# Build generator\n","\n","1.   we have two input layers, one for the noise z and one for the labels.\n","\n","2. An embedding layer is used to encode the label input into a 100-dimensional vector, i.e. one per latent dimension.\n","\n","3.  we then use the multiply layer to condition the noise on \"label_embedding\". This is referred to as model input.\n","\n","4.  This \"model input\" is fed through 3 hidden layers.\n","\n","5.   We also use batch normalisation after the first, second and third layer, to discourage overfitting.\n","\n","6.  Finally, we have one output layer with 67 nodes, each outputting one feature.\n","\n","7.  Note: we intentionally do not compile the generator model as it is not trained directly.\n","\n","\n","\n"],"metadata":{"id":"3LZoqkLhhXv4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2OxZ6TVAK7X3"},"outputs":[],"source":["def define_generator(latent_dim=100, out_shape=67,num_classes=2):\n","    # Initialiser that generates tensors with a normal distribution\n","    init = RandomNormal(mean=0.0, stddev=0.02)\n","    \n","    # label input layer \n","    label = Input(shape=(1,), dtype='int64', name=\"Label_input_layer\")\n","\n","    # convert label into 100 dimensional vector\n","    label_embedding = Flatten()(Embedding(num_classes, latent_dim)(label))\n","    \n","    # Noise z input layer\n","    noise = Input(shape=(latent_dim,),name=\"Noise_input_layer\")\n","\n","    # We condition the generation of features\n","    model_input = multiply([noise, label_embedding])\n","   \n","    # our first hidden layer and leaky relu activation\n","    fe = Dense(68, kernel_initializer=init, name=\"First_hidden_layer\")(model_input)\n","    fe = LeakyReLU(alpha=0.2)(fe)\n","    \n","    # apply batch normalisation\n","    fe = BatchNormalization(momentum=0.8)(fe)\n","\n","    # our second hidden layer and leaky relu activation\n","    fe = Dense(256, kernel_initializer=init, name=\"Second_hidden_layer\")(fe)\n","    fe = LeakyReLU(alpha=0.2)(fe)\n","\n","    # apply batch normalisation\n","    fe = BatchNormalization(momentum=0.8)(fe)\n","\n","    # our third hidden layer and leaky relu activation\n","    fe = Dense(units=512, kernel_initializer=init, name=\"Third_hidden_layer\")(fe)\n","    fe = LeakyReLU(alpha=0.2)(fe)\n","\n","    # apply batch normalisation\n","    fe = BatchNormalization(momentum=0.5)(fe)\n","\n","    # our output layer, with 67 nodes (one node per feature)\n","    out_layer= Dense(out_shape, activation='tanh',name=\"Output_layer\") (fe)\n","\n","    # define the generator model\n","    model = Model([noise, label], out_layer) \n","    return model\n"]},{"cell_type":"markdown","source":["# Combine the generator and discriminator models to form cGAN\n","\n","\"define_cgan()\" function combines our predefined generator and discriminator by taking them DDoSh as inputs."],"metadata":{"id":"iLu0_yelfDsq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bywtmqEbLEOZ"},"outputs":[],"source":["# define the combined generator and discriminator model, for updating the generator\n","def define_cgan(g_model, d_model):\n","\t# make weights in the discriminator not trainable\n","\td_model.trainable = False\n","\t\n","\t# get noise and label inputs from generator model\n","\tgen_noise, gen_label = g_model.input\n","\t\n","\t# get output from the generator model\n","\tgen_output = g_model.output\n","\t\n","\t# connect output and label input from generator as inputs to discriminator\n","\tgan_output = d_model([gen_output, gen_label])\n","\t\n","\t# define cgan model as taking noise and labels and outputting a classification\n","\tmodel = Model([gen_noise, gen_label], gan_output)\n"," \n","\t# compile model\n","\topt = Adam(lr=0.0002, beta_1=0.5)\n","\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n","\n","\t# save model architecture plot\n","\tplot_model(model, to_file='/content/drive/MyDrive/RESEARCH-PROJECT/Binary_experiments/Models/model_plotgan.png', show_shapes=True, show_layer_names=True)\n","\treturn model"]},{"cell_type":"markdown","source":["# Load and select real data samples\n","\n","Here we are selecting and returning a random sample of the real data and we assign it an initial class label for the discriminator i.e. we specify that this is a real observation, thus, belongs to y=1. \n","\n","Note: this \"y=1\" is to indicate a real observation, thus, here we are not indicating that it is either benign or DDoS. The benign or DDoS class is referred to as \"labels\"."],"metadata":{"id":"At49TLjof1O6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnfEo9U_LGBw"},"outputs":[],"source":["# load the real data samples\n","def load_real_samples(X,y):\n","\tprint(X.shape, y.shape)\n","\treturn [X, y]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4o_po1HLI5y"},"outputs":[],"source":["# select real samples\n","def generate_real_samples(dataset, n_samples):\n","    # split into observations  and labels\n","    features, labels = dataset\n","    # choose random instances\n","    rand = randint(0, 1000)\n","    # select observations and labels\n","    X = features.sample(n=n_samples,random_state=rand)\n","    labels = labels.sample(n=n_samples,random_state=rand)\n","\t  # generate labels\n","    y = ones((n_samples, 1))\n","    return [X, labels], y"]},{"cell_type":"markdown","source":["# Generate points in the latent space\n","\n","\"generate_latent_points()\" function takes in as an argument:\n","\n","\n","1. the size of the latent space \n","2. the number of points required \n","3. the number of classes i.e. 0 benign and 1 DDoS\n","\n","Then, it returns a batch of input samples for the generator model\n","\n","\n"],"metadata":{"id":"zVSHbpFLgE_d"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTBUQLx8LMoe"},"outputs":[],"source":["# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples, n_classes=2):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tz_input = x_input.reshape(n_samples, latent_dim)\n","\t# generate labels\n","\tlabels = randint(0, n_classes, n_samples)\n","\treturn [z_input, labels]"]},{"cell_type":"markdown","source":["# Use points as inputs to generator \n","\n","Here, we utilise the points in the latent space as input to the generator.\n","\n","In order to generate new observations we use the \"generate_fake_samples()\" function below. This takes as an argument:\n","\n","\n","\n","1.   the generator model \n","2.   the size of the latent space \n","\n","\n","Then, it generates points in the latent space and uses them as input to the generator model.\n","\n","Subsequently, the function returns the generated observations and their corresponding class label for the discriminator model, specifically y=0, indicating that they are fake/ generated."],"metadata":{"id":"Z7n8m9lIHtOh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvhaGpaILPzz"},"outputs":[],"source":["# use the generator to generate fake examples, with labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","\t# generate points in latent space\n","\tz_input, labels_input = generate_latent_points(latent_dim, n_samples)\n","\t# predict outputs\n","\tobservations = generator.predict([z_input, labels_input])\n","\t# create labels\n","\ty = zeros((n_samples, 1))\n","\treturn [observations , labels_input], y"]},{"cell_type":"markdown","source":["# Define training process\n","\n","At this stage we are ready to fit the cGAN model.\n","\n","The model is fit for 1000 training epochs and batch size of 1000 samples. These sizes are selected arbitrarily.\n","\n","**The train() function takes in as arguments:**\n","\n","\n","1.   the defined generator, discriminator and cGAN models\n","2.   the dataset\n","3.   the size of the latent dimension\n","4.   the number of epochs and batch size \n","\n","Then, the generator model is saved after each epoch.\n","\n","\n","\n","\n","\n","\n","\n","\n","**The training process is performed as follows:**\n","\n","1. the discriminator is updated using a half batch of real samples and then a half batch of fake samples. Thus, together they form one batch of weight updates\n","\n","2.   the generator is then updated via the composite gan model\n","\n","3. note: the labels are set to 1 for real and 0 for fake samples. This updates the generator towards getting better at synthesising real looking samples on the next batch\n"],"metadata":{"id":"zj-0da-_gMcF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaG3YvxMLl62"},"outputs":[],"source":["# train the generator and discriminator\n","def train(g_model, d_model, cgan_model, dataset, latent_dim, n_epochs=1000, n_batch=1000):\n","\tbat_per_epo = int(dataset[0].shape[0] / n_batch)\n","\thalf_batch = int(n_batch / 2)\n","\t# manually enumerate epochs\n","\tfor i in range(n_epochs):\n","\t\t# enumerate batches over the training set\n","\t\tfor j in range(bat_per_epo):\n","\t\t\t# get randomly selected 'real' samples\n","\t\t\t[X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n","\t\t\t# update discriminator model weights\n","\t\t\td_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n","\t\t\t# generate 'fake' examples\n","\t\t\t[X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","\t\t\t# update discriminator model weights\n","\t\t\td_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n","\t\t\t# prepare points in latent space as input for the generator\n","\t\t\t[z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n","\t\t\t# create inverted labels for the fake samples\n","\t\t\ty_gan = ones((n_batch, 1))\n","\t\t\t# update the generator via the discriminator's error\n","\t\t\tg_loss = cgan_model.train_on_batch([z_input, labels_input], y_gan)\n","\t\t\t# summarise loss on this batch\n","\t\t\tprint('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n","\t\t\t\t(i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n","\t \n","\t\t\tif i >= 1:\n","\t\t\t g_model.save('/content/drive/MyDrive/RESEARCH-PROJECT/Binary_experiments/Generated models/ddos models/g_model_%04d.h5' % (i))"]},{"cell_type":"markdown","source":["# Begin training process"],"metadata":{"id":"O9_Y5xrXgQnK"}},{"cell_type":"code","source":["# size of the latent space\n","latent_dim = 100\n","# create the discriminator\n","d_model = define_discriminator()\n","# create the generator\n","g_model = define_generator(latent_dim=latent_dim)\n","# create the cgan\n","cgan_model = define_cgan(g_model, d_model)\n","# load real observations data\n","dataset = load_real_samples(X_train,y_train)\n","# train model\n","train(g_model, d_model, cgan_model, dataset, latent_dim)"],"metadata":{"id":"2bvzd_SVtESg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find the best cGAN model post training \n","\n","Here we iterate over all saved models to assess the quality of the models. More precisely, we train a Random Forest on a generated sample and predict real data. The scores are then stored in a dictionary."],"metadata":{"id":"tndyhNFI_WmY"}},{"cell_type":"code","source":["# define random forest classifier\n","cgan_rf=RandomForestClassifier(max_depth=8,n_estimators=100)"],"metadata":{"id":"VK6xiYVW_WmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples, n_classes=2):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tz_input = x_input.reshape(n_samples, latent_dim)\n","\t# generate labels\n","\tlabels = randint(0, n_classes, n_samples)\n","\treturn [z_input, labels]"],"metadata":{"id":"59DwOKdv_WmZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_points_per_class(latentpoints, size):\n","    # generate latent points \n","    latent_points0, labels = generate_latent_points(latentpoints, size)\n","    latent_points1, labels = generate_latent_points(latentpoints, size)\n","    \n","    # create labels\n","    labels_0 = asarray([0  for _ in range(size)])\n","    labels_1= asarray([1  for _ in range(size)])\n","    \n","    return latent_points0 ,labels_0 , latent_points1, labels_1\n"],"metadata":{"id":"X8UI3Jj8_WmZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oqwj7-Ix_EDQ"},"outputs":[],"source":["def test_initial_quality(steps,model_path,latentpoints,size):\n","    scores={}\n","    for x in range(1,steps,1):\n","      # load model\n","      model = load_model(model_path % x)\n","\n","      # generate latent points and labels \n","      latent_points0 ,labels_0 , latent_points1, labels_1 = generate_points_per_class(latentpoints, size)\n","\n","      # generate data\n","      X_0  = model.predict([latent_points0, labels_0])\n","      X_1  = model.predict([latent_points1, labels_1])\n","\n","      # convert generated data into dataframes\n","      gen_df_0 = pd.DataFrame(data = X_0,columns = X_train.columns)\n","      gen_df_1 = pd.DataFrame(data = X_1,\n","                          columns = X_train.columns)\n","      \n","      # add labels to frames\n","      gen_df_0['attack_map']=0\n","      gen_df_1['attack_map']=1\n","\n","      # combine dataframes together \n","      df_gan = pd.concat([gen_df_0,gen_df_1], ignore_index=True, sort=False)\n","   \n","      # train random forest model on generated data\n","      cgan_rf.fit(df_gan.drop('attack_map',axis=1), df_gan['attack_map'])\n","      \n","      # predict real 2017 data sample\n","      y_pred = cgan_rf.predict(X_test)\n","\n","      # print classification results \n","      print(classification_report(y_test,y_pred))\n","      \n","      # store results in dict\n","      scores.update({x: f1_score(y_test, y_pred,average='micro')})\n","    return scores "]},{"cell_type":"markdown","source":["# Save generated data from our best model\n","\n","Finally, the best performing model is then retrieved and data generated from said model is saved for more rigorous testing of quality in a different notebook."],"metadata":{"id":"1-y48nJ3-38B"}},{"cell_type":"code","source":["# find the model with the highest f1 score within the scores dict\n","def keywithmaxval(dict1):\n","     \"\"\" a) create a list of the dict's keys and values; \n","         b) return the key with the max value\"\"\"  \n","     v=list(dict1.values())\n","     k=list(dict1.keys())\n","     return int(k[v.index(max(v))])"],"metadata":{"id":"R5lQOWdJ-38B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_best_data(scores,model_path,latentpoints, size,save_path):\n","    best_model = keywithmaxval(scores)\n","    model = load_model(model_path % best_model)\n","\n","    # generate latent points and labels \n","    latent_points0 ,labels_0 , latent_points1, labels_1 = generate_points_per_class(latentpoints, size)\n","\n","    # generate data\n","    X_0  = model.predict([latent_points0, labels_0])\n","    X_1  = model.predict([latent_points1, labels_1])\n","\n","    # convert generated data into dataframes\n","    gen_df_0 = pd.DataFrame(data = X_0,columns = X_train.columns)\n","    gen_df_1 = pd.DataFrame(data = X_1,\n","                        columns = X_train.columns)\n","\n","    # add labels to frames\n","    gen_df_0['attack_map']=0\n","    gen_df_1['attack_map']=1\n","\n","    # combine dataframes together \n","    df_gan = pd.concat([gen_df_0,gen_df_1], ignore_index=True, sort=False)\n","    \n","    # save best generated data \n","    df_gan.to_parquet(f'{save_path}', index=False)"],"metadata":{"id":"QzarJWGB-38B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path='/content/drive/MyDrive/RESEARCH-PROJECT/Binary_experiments/Generated models/ddos models/g_model_%04d.h5'\n","save_path='/content/drive/MyDrive/RESEARCH-PROJECT/Binary_experiments/Generated data/DDoS data/2017GENERATED_DDoS.parquet'\n","scores = test_initial_quality(1000,model_path,100, 1000)\n","save_best_data(scores,model_path,100, 10000,save_path)"],"metadata":{"id":"1Oyfoav4s_hC"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"DDoS-CICIDS2017 .ipynb","provenance":[{"file_id":"1yB1tRVa04csEdiMLMT4vK5Hcoa6Yo97Q","timestamp":1647115199369},{"file_id":"1pdHr5GqxSn2-atfWdwTbmQ1bOuWr5XIW","timestamp":1647115037065},{"file_id":"1dvZCDx8iWnrNpJ6xkI5gi8QA1blZQ3bT","timestamp":1647113703349},{"file_id":"1gku6vOVjRyJd__xm6E1wQsrdSNX6SEEt","timestamp":1647097776635},{"file_id":"13OBY6H-tuDAfgum6Md-BBii0VOJ4NbKu","timestamp":1646857301219},{"file_id":"1sO8d_hMewAgpBImVXBMSSB9F68jT6s8C","timestamp":1642794833364},{"file_id":"1Zwm9j0i9z9z5aDsSdPwDnt4gIzRU1zC9","timestamp":1641755658509},{"file_id":"17HZv-An8KnSRQKa1cWCK1cdC1DSbFT9r","timestamp":1641744454232},{"file_id":"1qLFDkWS6737wqFMuMSKSaF3-F1WNed4V","timestamp":1641735472527},{"file_id":"1TK3J8hQNKBfA5pFyJm-ZreAgvjhZAl2X","timestamp":1641646851946},{"file_id":"1FOb7gRkbWi1Uj1Wx6O_1UrbL_A6da2TN","timestamp":1641160747698},{"file_id":"1gHp0AenQ117Sq5fclIdePn8nw4BjZSrX","timestamp":1641145553719}],"mount_file_id":"13OBY6H-tuDAfgum6Md-BBii0VOJ4NbKu","authorship_tag":"ABX9TyN3/BEcaTsZrQETgZm5oSKA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}